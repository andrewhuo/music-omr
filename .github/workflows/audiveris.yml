name: Audiveris OMR + Measure Label Debug

on:
  workflow_dispatch:
    inputs:
      pdf_gcs_uri:
        description: "GCS URI to input PDF (gs://music-omr-bucket-777135743132/input/test.pdf)"
        required: true
        type: string

jobs:
  pipeline:
    runs-on: ubuntu-latest
    env:
      OUTPUT_PREFIX: "gs://music-omr-bucket-777135743132/output"
      AUDIVERIS_IMAGE: "ghcr.io/andrewhuo/audiveris-engine@sha256:61c2a4a39e6f545bb9153a984247b83ace541196c4861329b295982e4cf57bed"
      AUDIVERIS_VERSION: "5.9.0"
      MEASURE_NUMBERING_POLICY: "semantic_continuous_v1"

    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ secrets.GCP_WORKLOAD_IDENTITY_PROVIDER }}
          service_account: ${{ secrets.GCP_SERVICE_ACCOUNT_EMAIL }}
          create_credentials_file: true

      - name: Set up Google Cloud SDK (apt)
        run: |
          set -euo pipefail
          if command -v gcloud >/dev/null 2>&1; then
            gcloud --version
            exit 0
          fi
          sudo apt-get update -y
          sudo apt-get install -y apt-transport-https ca-certificates gnupg curl
          curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee /etc/apt/sources.list.d/google-cloud-sdk.list
          sudo apt-get update -y
          sudo apt-get install -y google-cloud-cli
          gcloud --version

      - name: Configure gcloud auth for gsutil
        run: |
          set -euo pipefail
          if [[ -z "${GOOGLE_APPLICATION_CREDENTIALS:-}" ]]; then
            echo "ERROR: GOOGLE_APPLICATION_CREDENTIALS not set"
            exit 1
          fi
          echo "GOOGLE_APPLICATION_CREDENTIALS=${GOOGLE_APPLICATION_CREDENTIALS}"
          echo "CLOUDSDK_AUTH_CREDENTIAL_FILE_OVERRIDE=${GOOGLE_APPLICATION_CREDENTIALS}" >> "$GITHUB_ENV"
          gcloud auth login --cred-file="${GOOGLE_APPLICATION_CREDENTIALS}"
          gcloud auth list

      - name: Show run context
        run: |
          set -euo pipefail
          RUN_OUTPUT_PREFIX="${OUTPUT_PREFIX}/runs/${GITHUB_RUN_ID}"
          echo "repo: $GITHUB_REPOSITORY"
          echo "sha:  $GITHUB_SHA"
          echo "ref:  ${GITHUB_REF_NAME:-$GITHUB_REF}"
          echo "run_id: ${GITHUB_RUN_ID}"
          echo "run_attempt: ${GITHUB_RUN_ATTEMPT}"
          echo "pdf_gcs_uri: ${{ inputs.pdf_gcs_uri }}"
          echo "audiveris_image: ${AUDIVERIS_IMAGE}"
          echo "output_prefix: ${OUTPUT_PREFIX}"
          echo "run_output_prefix: ${RUN_OUTPUT_PREFIX}"

      - name: Download input PDF
        run: |
          set -euo pipefail
          mkdir -p /tmp/work
          gsutil cp "${{ inputs.pdf_gcs_uri }}" /tmp/work/input.pdf
          ls -lh /tmp/work/input.pdf

      - name: Run Audiveris OMR
        run: |
          set -euo pipefail
          mkdir -p /tmp/work
          mkdir -p /tmp/work/.cache /tmp/work/.config /tmp/work/tmp
          AUDIVERIS_IMAGE_EFFECTIVE="${AUDIVERIS_IMAGE}"

          echo "WORKFLOW_SIGNATURE=engine-pin-v1"
          echo "WORKFLOW_SHA=${GITHUB_SHA}"
          echo "AUDIVERIS_IMAGE=${AUDIVERIS_IMAGE_EFFECTIVE}"

          if [[ -z "${AUDIVERIS_IMAGE_EFFECTIVE}" ]]; then
            echo "ERROR: AUDIVERIS_IMAGE is empty"
            exit 1
          fi
          if [[ "${AUDIVERIS_IMAGE_EFFECTIVE}" != audiveris-local:* && "${AUDIVERIS_IMAGE_EFFECTIVE}" != *@sha256:* ]]; then
            echo "ERROR: audiveris_image must be pinned by digest using @sha256: (or be audiveris-local:<version>)"
            exit 1
          fi
          if [[ "${AUDIVERIS_IMAGE_EFFECTIVE}" == audiveris-local:* ]]; then
            echo "PULLING_AUDIVERIS_IMAGE=skipped_local_build"
          else
            echo "PULLING_AUDIVERIS_IMAGE=${AUDIVERIS_IMAGE_EFFECTIVE}"
            docker pull "${AUDIVERIS_IMAGE_EFFECTIVE}"
          fi
          echo "AUDIVERIS_IMAGE_ID=$(docker image inspect "${AUDIVERIS_IMAGE_EFFECTIVE}" --format '{{.Id}}')"
          echo "AUDIVERIS_IMAGE_REPODIGESTS=$(docker image inspect "${AUDIVERIS_IMAGE_EFFECTIVE}" --format '{{json .RepoDigests}}')"

          probe_omr_version() {
            local image="$1"
            local required="5.9.0"
            docker run --rm \
              --entrypoint /bin/sh \
              --user "$(id -u):$(id -g)" \
              -v /tmp/work:/work \
              -e HOME="/work" \
              -e XDG_CACHE_HOME="/work/.cache" \
              -e XDG_CONFIG_HOME="/work/.config" \
              -e JAVA_TOOL_OPTIONS="-Duser.home=/work -Djava.io.tmpdir=/work/tmp" \
              "$image" \
              -c 'set -e; AUDI_BIN="/audiveris-extract/bin/Audiveris"; if [ -x /audiveris/bin/Audiveris ]; then AUDI_BIN="/audiveris/bin/Audiveris"; fi; if [ -x /opt/audiveris/bin/Audiveris ]; then AUDI_BIN="/opt/audiveris/bin/Audiveris"; fi; mkdir -p /work/.cache /work/.config /work/tmp; "$AUDI_BIN" -help >/work/audi_help_probe.txt 2>&1 || true;'

            if [[ ! -f /tmp/work/audi_help_probe.txt ]]; then
              echo "ERROR: probe output missing /tmp/work/audi_help_probe.txt"
              exit 1
            fi

            local version_raw
            version_raw="$(grep -Eo '([0-9]+\.[0-9]+\.[0-9]+([-.:][A-Za-z0-9]+)?)' /tmp/work/audi_help_probe.txt | head -n 1 || true)"
            local version_num
            version_num="$(grep -Eo '[0-9]+\.[0-9]+\.[0-9]+' /tmp/work/audi_help_probe.txt | head -n 1 || true)"

            echo "AUDIVERIS_VERSION_RAW=${version_raw}"
            echo "AUDIVERIS_VERSION_NUM=${version_num}"

            if [[ -z "${version_num}" ]]; then
              echo "ERROR: unable to detect Audiveris version from -help output"
              tail -n 120 /tmp/work/audi_help_probe.txt || true
              exit 1
            fi
            echo "AUDIVERIS_HELP_PROBE_HEAD_BEGIN"
            sed -n '1,80p' /tmp/work/audi_help_probe.txt || true
            echo "AUDIVERIS_HELP_PROBE_HEAD_END"

            if [[ "$(printf '%s\n%s\n' "${required}" "${version_num}" | sort -V | head -n 1)" != "${required}" ]]; then
              echo "ERROR: Audiveris version ${version_num} is below required ${required}"
              exit 1
            fi
          }

          run_omr() {
            local image="$1"
            local out_dir_name="$2"
            docker run --rm \
              --entrypoint /bin/sh \
              --user "$(id -u):$(id -g)" \
              -v /tmp/work:/work \
              -e AUDI_IMAGE="${image}" \
              -e AUDI_OUT_DIR="/work/${out_dir_name}" \
              -e HOME="/work" \
              -e XDG_CACHE_HOME="/work/.cache" \
              -e XDG_CONFIG_HOME="/work/.config" \
              -e JAVA_TOOL_OPTIONS="-Duser.home=/work -Djava.io.tmpdir=/work/tmp" \
              "$image" \
              -c 'set -e; AUDI_BIN="/audiveris-extract/bin/Audiveris"; if [ -x /audiveris/bin/Audiveris ]; then AUDI_BIN="/audiveris/bin/Audiveris"; fi; if [ -x /opt/audiveris/bin/Audiveris ]; then AUDI_BIN="/opt/audiveris/bin/Audiveris"; fi; mkdir -p /work/.cache /work/.config /work/tmp; echo "AUDIVERIS_ATTEMPT image=${AUDI_IMAGE} out_dir=${AUDI_OUT_DIR} bin=${AUDI_BIN} HOME=${HOME} XDG_CACHE_HOME=${XDG_CACHE_HOME}"; "$AUDI_BIN" -help >/work/audi_help.txt 2>&1 || true; if grep -Eq "AccessDeniedException: /\\.cache|Loader\\.getCacheDir\\(\\).*null|ExceptionInInitializerError" /work/audi_help.txt; then echo "AUDIVERIS_PREFLIGHT_ERROR"; tail -n 120 /work/audi_help.txt || true; exit 86; fi; "$AUDI_BIN" -batch -export -output "$AUDI_OUT_DIR" /work/input.pdf'
          }

          run_omr_smoke() {
            local image="$1"
            local out_dir_name="$2"
            docker run --rm \
              --entrypoint /bin/sh \
              --user "$(id -u):$(id -g)" \
              -v /tmp/work:/work \
              -e AUDI_IMAGE="${image}" \
              -e AUDI_OUT_DIR="/work/${out_dir_name}" \
              -e HOME="/work" \
              -e XDG_CACHE_HOME="/work/.cache" \
              -e XDG_CONFIG_HOME="/work/.config" \
              -e JAVA_TOOL_OPTIONS="-Duser.home=/work -Djava.io.tmpdir=/work/tmp" \
              "$image" \
              -c 'set -e; AUDI_BIN="/audiveris-extract/bin/Audiveris"; if [ -x /audiveris/bin/Audiveris ]; then AUDI_BIN="/audiveris/bin/Audiveris"; fi; if [ -x /opt/audiveris/bin/Audiveris ]; then AUDI_BIN="/opt/audiveris/bin/Audiveris"; fi; mkdir -p /work/.cache /work/.config /work/tmp; echo "AUDIVERIS_SMOKE image=${AUDI_IMAGE} out_dir=${AUDI_OUT_DIR} bin=${AUDI_BIN}"; echo "AUDIVERIS_SMOKE_CMD=\"${AUDI_BIN}\" -batch -export -output \"${AUDI_OUT_DIR}\" -sheets 1 -- /work/input.pdf"; "$AUDI_BIN" -batch -export -output "$AUDI_OUT_DIR" -sheets 1 -- /work/input.pdf'
          }

          check_mxl() {
            local out_dir="$1"
            local context="${2:-unknown}"
            python - "$out_dir" "$context" <<'PY'
          import glob
          import os
          import re
          import sys
          import zipfile
          import xml.etree.ElementTree as ET

          try:
              from lxml import etree as LET  # type: ignore
          except Exception:
              LET = None


          def local(tag):
              if isinstance(tag, str):
                  return tag.split("}", 1)[-1] if "}" in tag else tag
              return str(tag)


          def count_by_tagscan(raw: bytes):
              parts = len(re.findall(br"<\s*part(\s|>)", raw, flags=re.IGNORECASE))
              measures = len(re.findall(br"<\s*measure(\s|>)", raw, flags=re.IGNORECASE))
              return parts, measures


          def score_xml_member(raw: bytes):
              try:
                  root = ET.fromstring(raw)
                  parts = sum(1 for el in root.iter() if local(el.tag) == "part")
                  measures = sum(1 for el in root.iter() if local(el.tag) == "measure")
                  return {
                      "parse": "stdlib",
                      "root": local(root.tag),
                      "parts": parts,
                      "measures": measures,
                  }
              except Exception:
                  pass

              if LET is not None:
                  try:
                      parser = LET.XMLParser(recover=True, huge_tree=True)
                      root = LET.fromstring(raw, parser=parser)
                      root_name = local(root.tag) if root is not None else "unknown"
                      parts = len(root.xpath('//*[local-name()="part"]')) if root is not None else 0
                      measures = len(root.xpath('//*[local-name()="measure"]')) if root is not None else 0
                      return {
                          "parse": "lxml_recover",
                          "root": root_name,
                          "parts": parts,
                          "measures": measures,
                      }
                  except Exception:
                      pass

              parts, measures = count_by_tagscan(raw)
              return {
                  "parse": "tagscan",
                  "root": "unknown",
                  "parts": parts,
                  "measures": measures,
              }


          def score_mxl(mxl_path):
              best_member = None
              try:
                  with zipfile.ZipFile(mxl_path, "r") as z:
                      for name in z.namelist():
                          if name.startswith("META-INF/"):
                              continue
                          if not (name.lower().endswith(".xml") or name.lower().endswith(".musicxml")):
                              continue
                          try:
                              raw = z.read(name)
                          except Exception:
                              continue
                          scored = score_xml_member(raw)
                          cand = {"member": name, **scored}
                          if best_member is None or (cand["measures"], cand["parts"]) > (
                              best_member["measures"],
                              best_member["parts"],
                          ):
                              best_member = cand
              except Exception as exc:
                  return {
                      "path": mxl_path,
                      "member": None,
                      "parse": "zip_error",
                      "root": "zip_error",
                      "parts": 0,
                      "measures": 0,
                      "error": str(exc),
                  }

              if best_member is None:
                  return {
                      "path": mxl_path,
                      "member": None,
                      "parse": "none",
                      "root": "no_xml_members",
                      "parts": 0,
                      "measures": 0,
                      "error": None,
                  }

              return {"path": mxl_path, **best_member, "error": None}


          out_dir = sys.argv[1]
          context = sys.argv[2]
          search_roots = [
              os.path.join(out_dir, "*.mxl"),
              os.path.join(out_dir, "input", "*.mxl"),
          ]
          mxl_candidates = sorted({path for pattern in search_roots for path in glob.glob(pattern)})
          print(f"MXL_CHECK_SEARCH_ROOTS context={context} roots={search_roots}")
          print(f"MXL_CHECK_CONTEXT context={context} out_dir={out_dir} candidate_count={len(mxl_candidates)}")
          if not mxl_candidates:
              print(f"MXL_CHECK_FAIL context={context} reason=no_mxl_files out_dir={out_dir}")
              raise SystemExit(2)

          scored = [score_mxl(path) for path in mxl_candidates]
          for row in scored:
              err = f" error={row['error']}" if row.get("error") else ""
              print(
                  "MXL_CANDIDATE "
                  f"context={context} path={row['path']} member={row['member']} parse={row['parse']} "
                  f"root={row['root']} parts={row['parts']} measures={row['measures']}{err}"
              )

          best = max(scored, key=lambda r: (r["measures"], r["parts"]))
          print(
              "MXL_SELECTED "
              f"context={context} path={best['path']} member={best['member']} parse={best['parse']} "
              f"parts={best['parts']} measures={best['measures']}"
          )

          with open(f"/tmp/work/selected_mxl_path_{context}.txt", "w", encoding="utf-8") as f:
              f.write(best["path"])
          with open(f"/tmp/work/selected_mxl_member_{context}.txt", "w", encoding="utf-8") as f:
              f.write(best["member"] or "")
          if context == "full":
              with open("/tmp/work/selected_mxl_path.txt", "w", encoding="utf-8") as f:
                  f.write(best["path"])
              with open("/tmp/work/selected_mxl_member.txt", "w", encoding="utf-8") as f:
                  f.write(best["member"] or "")

          if best["parts"] <= 0 or best["measures"] <= 0:
              print(
                  "MXL_CHECK_FAIL "
                  f"context={context} reason=parts_or_measures_zero path={best['path']} "
                  f"member={best['member']} parts={best['parts']} measures={best['measures']}"
              )
              raise SystemExit(1)
          PY
          }

          probe_omr_version "${AUDIVERIS_IMAGE_EFFECTIVE}"

          SMOKE_RUN_EXIT_CODE=0
          if run_omr_smoke "${AUDIVERIS_IMAGE_EFFECTIVE}" audiveris_smoke; then
            SMOKE_RUN_EXIT_CODE=0
          else
            SMOKE_RUN_EXIT_CODE=$?
          fi

          SMOKE_MXL_OK=false
          if check_mxl "/tmp/work/audiveris_smoke" "smoke"; then
            SMOKE_MXL_OK=true
          fi

          if [[ "${SMOKE_RUN_EXIT_CODE}" -ne 0 || "${SMOKE_MXL_OK}" != "true" ]]; then
            echo "SMOKE_STATUS=warn"
            echo "SMOKE_RUN_EXIT_CODE=${SMOKE_RUN_EXIT_CODE}"
            echo "SMOKE_MXL_OK=${SMOKE_MXL_OK}"
            echo "SMOKE_BYPASS continuing_to_full_run=true"
            echo "Smoke outputs (warn path):"
            find /tmp/work/audiveris_smoke -maxdepth 4 -type f -print || true
          else
            echo "SMOKE_STATUS=ok"
            echo "SMOKE_RUN_EXIT_CODE=${SMOKE_RUN_EXIT_CODE}"
            echo "SMOKE_MXL_OK=${SMOKE_MXL_OK}"
            echo "SMOKE_SELECTED_PATH=$(cat /tmp/work/selected_mxl_path_smoke.txt)"
            echo "SMOKE_SELECTED_MEMBER=$(cat /tmp/work/selected_mxl_member_smoke.txt)"
          fi

          run_omr "${AUDIVERIS_IMAGE_EFFECTIVE}" audiveris_out_try1
          SELECTED_OUT="/tmp/work/audiveris_out_try1"
          if ! check_mxl "$SELECTED_OUT" "full"; then
            echo "MusicXML export has no parts/measures after successful startup."
            echo "Failing fast in xml-strict mode (no fallback image retry)."
            exit 1
          fi

          AUDI_SELECTED_OUT="${SELECTED_OUT}"
          OMR_PATH=""
          if [[ -f "${AUDI_SELECTED_OUT}/input/input.omr" ]]; then
            OMR_PATH="${AUDI_SELECTED_OUT}/input/input.omr"
          elif [[ -f "${AUDI_SELECTED_OUT}/input.omr" ]]; then
            OMR_PATH="${AUDI_SELECTED_OUT}/input.omr"
          else
            OMR_PATH="$(find "${AUDI_SELECTED_OUT}" -maxdepth 3 -type f -name "*.omr" | head -n 1 || true)"
          fi
          if [[ -z "${OMR_PATH}" || ! -f "${OMR_PATH}" ]]; then
            echo "ERROR: missing .omr output under ${AUDI_SELECTED_OUT}"
            find "${AUDI_SELECTED_OUT}" -maxdepth 4 -type f -print || true
            exit 1
          fi
          MXL_MEMBER_OVERRIDE="$(cat /tmp/work/selected_mxl_member.txt)"
          MXL_PATH_OVERRIDE="$(cat /tmp/work/selected_mxl_path.txt)"
          echo "AUDI_SELECTED_OUT=${AUDI_SELECTED_OUT}" >> "$GITHUB_ENV"
          echo "AUDI_OMR_PATH=${OMR_PATH}" >> "$GITHUB_ENV"
          echo "MXL_MEMBER_OVERRIDE=${MXL_MEMBER_OVERRIDE}" >> "$GITHUB_ENV"
          echo "MXL_PATH_OVERRIDE=${MXL_PATH_OVERRIDE}" >> "$GITHUB_ENV"
          echo "AUDI_SELECTED_OUT=${AUDI_SELECTED_OUT}"
          echo "AUDI_OMR_PATH=${OMR_PATH}"
          echo "MXL_PATH_OVERRIDE=${MXL_PATH_OVERRIDE}"
          echo "MXL_MEMBER_OVERRIDE=${MXL_MEMBER_OVERRIDE}"

          echo "Audiveris outputs:"
          find "${AUDI_SELECTED_OUT}" -maxdepth 4 -type f -print || true
          echo "Smoke outputs:"
          find /tmp/work/audiveris_smoke -maxdepth 4 -type f -print || true
          echo "MusicXML candidates:"
          ls -lh "${AUDI_SELECTED_OUT}/input" || true
          find "${AUDI_SELECTED_OUT}" -maxdepth 4 -type f \( -name "*.mxl" -o -name "*.musicxml" -o -name "*.xml" \) -print || true

      - name: MusicXML debug (member list + head)
        run: |
          set -euo pipefail
          MXL="${MXL_PATH_OVERRIDE:-}"
          MEMBER="${MXL_MEMBER_OVERRIDE:-}"
          if [[ -z "$MXL" && -f /tmp/work/selected_mxl_path.txt ]]; then
            MXL="$(cat /tmp/work/selected_mxl_path.txt)"
          fi
          if [[ -z "$MEMBER" && -f /tmp/work/selected_mxl_member.txt ]]; then
            MEMBER="$(cat /tmp/work/selected_mxl_member.txt)"
          fi
          if [[ ! -f "$MXL" ]]; then
            echo "No MXL at $MXL"
            find "${AUDI_SELECTED_OUT:-/tmp/work}" -maxdepth 4 -type f -print || true
            exit 0
          fi
          echo "=== MXL member list (xml/musicxml only) ==="
          unzip -l "$MXL" | grep -E '\.xml$|\.musicxml$' || true
          echo "=== Head of selected XML member ==="
          if [[ -n "$MEMBER" ]]; then
            echo "Selected member: $MEMBER"
            unzip -p "$MXL" "$MEMBER" | head -n 200 || true
          else
            echo "No selected member found, printing first xml member."
            FIRST_MEMBER="$(unzip -Z1 "$MXL" | grep -E '\.xml$|\.musicxml$' | head -n 1 || true)"
            if [[ -n "$FIRST_MEMBER" ]]; then
              echo "First member: $FIRST_MEMBER"
              unzip -p "$MXL" "$FIRST_MEMBER" | head -n 200 || true
            fi
          fi

      - name: Validate OMR and print archive debug
        run: |
          set -euo pipefail
          OMR="${AUDI_OMR_PATH:-}"
          if [[ -z "$OMR" ]]; then
            OMR="${AUDI_SELECTED_OUT}/input/input.omr"
          fi
          if [[ ! -f "$OMR" ]]; then
            echo "ERROR: missing $OMR"
            find "${AUDI_SELECTED_OUT}" -maxdepth 4 -type f -print || true
            exit 1
          fi

          echo "Top of OMR archive listing:"
          unzip -l "$OMR" | sed -n '1,220p'

          echo "Audiveris logs (tail):"
          for f in "${AUDI_SELECTED_OUT}"/*.log "${AUDI_SELECTED_OUT}"/input/*.log; do
            [[ -f "$f" ]] || continue
            echo "--- $f (last 120 lines) ---"
            tail -n 120 "$f" || true
          done

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Python dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install opencv-python==4.12.0.88 pymupdf==1.20.0 numpy lxml==5.3.0

      - name: Build page-split XML manifest (strict mode)
        run: |
          set -euo pipefail
          mkdir -p /tmp/work/pages /tmp/work/audiveris_pages /tmp/work/page_logs
          AUDIVERIS_IMAGE_EFFECTIVE="${AUDIVERIS_IMAGE}"

          python - << 'PY'
          import fitz
          import os

          src = "/tmp/work/input.pdf"
          out_dir = "/tmp/work/pages"
          os.makedirs(out_dir, exist_ok=True)

          doc = fitz.open(src)
          for idx in range(doc.page_count):
              single = fitz.open()
              single.insert_pdf(doc, from_page=idx, to_page=idx)
              out_path = os.path.join(out_dir, f"page_{idx+1:04d}.pdf")
              single.save(out_path)
              single.close()
          doc.close()
          print(f"PAGE_SPLIT total_pages={len([p for p in os.listdir(out_dir) if p.endswith('.pdf')])}")
          PY

          run_page_omr() {
            local image="$1"
            local page_num="$2"
            docker run --rm \
              --entrypoint /bin/sh \
              --user "$(id -u):$(id -g)" \
              -v /tmp/work:/work \
              -e PAGE_NUM="${page_num}" \
              -e AUDI_IMAGE="${image}" \
              -e AUDI_OUT_DIR="/work/audiveris_pages/page_${page_num}" \
              -e HOME="/work" \
              -e XDG_CACHE_HOME="/work/.cache" \
              -e XDG_CONFIG_HOME="/work/.config" \
              -e JAVA_TOOL_OPTIONS="-Duser.home=/work -Djava.io.tmpdir=/work/tmp" \
              "${image}" \
              -c 'set -e; AUDI_BIN="/audiveris-extract/bin/Audiveris"; if [ -x /audiveris/bin/Audiveris ]; then AUDI_BIN="/audiveris/bin/Audiveris"; fi; if [ -x /opt/audiveris/bin/Audiveris ]; then AUDI_BIN="/opt/audiveris/bin/Audiveris"; fi; mkdir -p /work/.cache /work/.config /work/tmp "${AUDI_OUT_DIR}"; "$AUDI_BIN" -batch -export -output "${AUDI_OUT_DIR}" "/work/pages/page_${PAGE_NUM}.pdf"'
          }

          for page_pdf in /tmp/work/pages/page_*.pdf; do
            [[ -f "${page_pdf}" ]] || continue
            page_num="$(basename "${page_pdf}" | sed -E 's/^page_([0-9]{4})\.pdf$/\1/')"
            log_path="/tmp/work/page_logs/page_${page_num}.log"
            if run_page_omr "${AUDIVERIS_IMAGE_EFFECTIVE}" "${page_num}" >"${log_path}" 2>&1; then
              exit_code=0
            else
              exit_code=$?
            fi
            echo "PAGE_XML_RUN page=$((10#${page_num})) image=${AUDIVERIS_IMAGE_EFFECTIVE} exit_code=${exit_code} log=${log_path}"
          done

          python - << 'PY'
          import glob
          import json
          import os
          import re
          import zipfile
          import xml.etree.ElementTree as ET

          MANIFEST_PATH = "/tmp/work/mxl_page_manifest.json"
          OMR_PATH = os.environ.get("AUDI_OMR_PATH", "").strip()
          NUMBERING_POLICY = (os.environ.get("MEASURE_NUMBERING_POLICY", "semantic_continuous_v1") or "").strip() or "semantic_continuous_v1"
          if NUMBERING_POLICY not in ("semantic_continuous_v1", "legacy_raw_starts"):
              print(
                  f"SEMANTIC_WARN page=- reason=invalid_numbering_policy:{NUMBERING_POLICY} fallback=semantic_continuous_v1"
              )
              NUMBERING_POLICY = "semantic_continuous_v1"

          def local(tag):
              if isinstance(tag, str):
                  return tag.split("}", 1)[-1] if "}" in tag else tag
              return str(tag)

          def iter_named(root, name):
              out = []
              for el in root.iter():
                  tag = getattr(el, "tag", None)
                  if isinstance(tag, str) and local(tag) == name:
                      out.append(el)
              return out

          def children_named(el, name):
              return [c for c in list(el) if local(c.tag) == name]

          def truthy_attr(v):
              if not v:
                  return False
              return str(v).strip().lower() in ("1", "true", "yes")

          def parse_ending_number_tokens(v):
              if not v:
                  return []
              out = []
              for tok in re.split(r"[^0-9]+", str(v)):
                  tok = tok.strip()
                  if tok:
                      out.append(tok)
              return out

          def parse_xml_int(v):
              txt = (v or "").strip()
              if not re.fullmatch(r"-?\d+", txt):
                  return None
              try:
                  return int(txt)
              except Exception:
                  return None

          def parse_xml_member(raw):
              try:
                  root = ET.fromstring(raw)
              except Exception:
                  parts = len(re.findall(br"<\s*part(\s|>)", raw, flags=re.IGNORECASE))
                  measures = len(re.findall(br"<\s*measure(\s|>)", raw, flags=re.IGNORECASE))
                  return {
                      "parse": "tagscan",
                      "root": "unknown",
                      "parts": parts,
                      "measures": measures,
                      "system_starts": [],
                      "measure_events": [],
                      "pickup_detected": False,
                      "repeat_forward_count": 0,
                      "repeat_backward_count": 0,
                      "ending_start_count": 0,
                  }

              parts = len(iter_named(root, "part"))
              measures = len(iter_named(root, "measure"))
              starts = []
              measure_events = []
              pickup_detected = False
              repeat_forward_count = 0
              repeat_backward_count = 0
              ending_start_count = 0

              if local(root.tag) == "score-partwise":
                  score_parts = children_named(root, "part")
                  if score_parts:
                      ms = children_named(score_parts[0], "measure")
                  else:
                      ms = []
                  if ms:
                      for i, m in enumerate(ms, start=1):
                          label = (m.get("number") or "").strip() or str(i)
                          measure_num_int = parse_xml_int(label)
                          implicit = truthy_attr(m.get("implicit"))
                          if i == 1 and implicit:
                              pickup_detected = True

                          is_new_system = (i == 1)
                          has_system_layout = False
                          for pr in children_named(m, "print"):
                              is_new_system = is_new_system or truthy_attr(pr.get("new-system")) or truthy_attr(pr.get("new-page"))
                              if children_named(pr, "system-layout"):
                                  has_system_layout = True
                          if is_new_system or has_system_layout:
                              starts.append(label)

                          has_forward_repeat = False
                          has_backward_repeat = False
                          ending_numbers = []
                          for bar in children_named(m, "barline"):
                              for rep in children_named(bar, "repeat"):
                                  direction = (rep.get("direction") or "").strip().lower()
                                  if direction == "forward":
                                      has_forward_repeat = True
                                  elif direction == "backward":
                                      has_backward_repeat = True
                              for ending in children_named(bar, "ending"):
                                  ending_type = (ending.get("type") or "").strip().lower()
                                  if ending_type in ("start", "discontinue"):
                                      ending_numbers.extend(parse_ending_number_tokens(ending.get("number")))
                                  if ending_type == "start":
                                      ending_start_count += 1

                          if has_forward_repeat:
                              repeat_forward_count += 1
                          if has_backward_repeat:
                              repeat_backward_count += 1

                          measure_events.append(
                              {
                                  "measure_index": i - 1,
                                  "label": label,
                                  "number_int": measure_num_int,
                                  "implicit": implicit,
                                  "system_start": bool(is_new_system or has_system_layout),
                                  "repeat_forward": has_forward_repeat,
                                  "repeat_backward": has_backward_repeat,
                                  "ending_start_numbers": ending_numbers,
                              }
                          )
              return {
                  "parse": "stdlib",
                  "root": local(root.tag),
                  "parts": parts,
                  "measures": measures,
                  "system_starts": starts,
                  "measure_events": measure_events,
                  "pickup_detected": pickup_detected,
                  "repeat_forward_count": repeat_forward_count,
                  "repeat_backward_count": repeat_backward_count,
                  "ending_start_count": ending_start_count,
              }

          def movement_index_for_path(path):
              base = os.path.basename(path)
              m = re.search(r"\.mvt(\d+)\.mxl$", base, flags=re.IGNORECASE)
              if m:
                  try:
                      return int(m.group(1))
                  except Exception:
                      return 1
              return 1

          def count_omr_systems_in_sheet(raw):
              try:
                  root = ET.fromstring(raw)
              except Exception:
                  return None

              pages = root.findall("page")
              if not pages:
                  return 0

              count = 0
              for page in pages:
                  systems = page.findall(".//system")
                  if not systems:
                      systems = page.findall("system")
                  count += len(systems)
              return count

          def load_omr_system_targets(omr_path, total_pages):
              targets = {}
              if not omr_path:
                  return targets, "missing_path", "env_AUDI_OMR_PATH_empty"
              if not os.path.exists(omr_path):
                  return targets, "missing_file", f"omr_not_found:{omr_path}"

              try:
                  with zipfile.ZipFile(omr_path, "r") as z:
                      names = z.namelist()
                      for page_num in range(1, total_pages + 1):
                          preferred = f"sheet#{page_num}/sheet#{page_num}.xml"
                          member = None
                          if preferred in names:
                              member = preferred
                          else:
                              for name in names:
                                  if name.endswith("/" + preferred) or name.endswith(preferred):
                                      member = name
                                      break
                          if not member:
                              continue
                          try:
                              raw = z.read(member)
                          except Exception:
                              continue
                          systems = count_omr_systems_in_sheet(raw)
                          if systems is None:
                              continue
                          targets[page_num] = int(systems)
              except Exception as exc:
                  return {}, "zip_error", str(exc)

              return targets, ("ok" if targets else "empty"), None

          def normalize_measure_label_to_int(label, allow_normalize=True):
              txt = str(label).strip()
              if re.fullmatch(r"-?\d+", txt):
                  return int(txt), False, "strict_numeric"
              if allow_normalize:
                  m = re.match(r"^\s*([+-]?\d+)", txt)
                  if m:
                      try:
                          num = int(m.group(1))
                      except Exception:
                          num = None
                      if num is not None:
                          return num, True, f"normalized_suffix_label:{txt}->{num}"
              return None, False, f"non_numeric_unparseable:{txt or '<empty>'}"

          def parse_int_labels(labels, allow_normalize=True):
              nums = []
              warnings = []
              for label in labels:
                  num, normalized, reason = normalize_measure_label_to_int(label, allow_normalize=allow_normalize)
                  if num is None:
                      warnings.append(reason)
                      return None, warnings
                  nums.append(int(num))
                  if normalized:
                      warnings.append(reason)
              return nums, warnings

          def strictly_increasing(nums):
              return all(b > a for a, b in zip(nums, nums[1:]))

          def score_mxl(mxl_path):
              best = None
              try:
                  with zipfile.ZipFile(mxl_path, "r") as z:
                      for name in z.namelist():
                          lname = name.lower()
                          if name.startswith("META-INF/"):
                              continue
                          if not (lname.endswith(".xml") or lname.endswith(".musicxml")):
                              continue
                          try:
                              raw = z.read(name)
                          except Exception:
                              continue
                          scored = parse_xml_member(raw)
                          cand = {"member": name, **scored}
                          if best is None or (cand["measures"], cand["parts"]) > (best["measures"], best["parts"]):
                              best = cand
              except Exception as exc:
                  return {
                      "path": mxl_path,
                      "member": None,
                      "parse": "zip_error",
                      "root": "zip_error",
                      "parts": 0,
                      "measures": 0,
                      "system_starts": [],
                      "measure_events": [],
                      "pickup_detected": False,
                      "repeat_forward_count": 0,
                      "repeat_backward_count": 0,
                      "ending_start_count": 0,
                      "error": str(exc),
                  }
              if best is None:
                  return {
                      "path": mxl_path,
                      "movement_index": movement_index_for_path(mxl_path),
                      "member": None,
                      "parse": "none",
                      "root": "no_xml_members",
                      "parts": 0,
                      "measures": 0,
                      "system_starts": [],
                      "measure_events": [],
                      "pickup_detected": False,
                      "repeat_forward_count": 0,
                      "repeat_backward_count": 0,
                      "ending_start_count": 0,
                      "error": None,
                  }
              return {
                  "path": mxl_path,
                  "movement_index": movement_index_for_path(mxl_path),
                  **best,
                  "system_count": len(best.get("system_starts") or []),
                  "error": None,
              }

          def select_manifest_entry(valid_rows, target_system_count):
              if target_system_count <= 0:
                  return None, "target_missing"

              normalized = []
              for row in valid_rows:
                  starts_raw = list(row.get("system_starts") or [])
                  nums, norm_warnings = parse_int_labels(starts_raw, allow_normalize=True)
                  if nums is None:
                      continue
                  if not strictly_increasing(nums):
                      continue
                  normalized.append((row, nums, starts_raw, norm_warnings))

              if not normalized:
                  return None, "no_strict_numeric_candidates"

              exact_rows = [
                  item
                  for item in normalized
                  if len(item[1]) == target_system_count
              ]
              if exact_rows:
                  chosen_row, chosen_nums, chosen_raw, chosen_warnings = max(
                      exact_rows,
                      key=lambda item: (
                          item[0]["measures"],
                          item[0]["parts"],
                          -int(item[0].get("movement_index") or 1),
                          item[0]["path"],
                          item[0].get("member") or "",
                      ),
                  )
                  return {
                      "kind": "single_exact",
                      "rows": [chosen_row],
                      "system_starts": [str(n) for n in chosen_nums],
                      "raw_system_starts": list(chosen_raw),
                      "system_starts_normalized": [str(n) for n in chosen_nums],
                      "normalization_warnings": list(chosen_warnings),
                      "row_offsets": [0],
                      "system_starts_raw_segments": [
                          {
                              "path": chosen_row.get("path"),
                              "movement_index": int(chosen_row.get("movement_index") or 1),
                              "system_starts": list(chosen_raw),
                          }
                      ],
                  }, None

              ordered = sorted(
                  normalized,
                  key=lambda item: (int(item[0].get("movement_index") or 1), item[0]["path"]),
              )
              merged_rows = []
              merged_nums = []
              merged_raw_labels = []
              row_offsets = []
              raw_segments = []
              any_shift = False
              merged_warnings = []
              for row, raw_nums, raw_starts, row_warnings in ordered:
                  offset = 0
                  if merged_nums:
                      prev_end = merged_nums[-1]
                      raw_first = raw_nums[0]
                      if raw_first > prev_end:
                          offset = 0
                      elif raw_first == 1:
                          # Deterministic reset-aware shift: keep next chunk contiguous.
                          offset = prev_end
                          any_shift = True
                      else:
                          return None, "merged_overlap_without_reset"

                  shifted = [n + offset for n in raw_nums]
                  if merged_nums and shifted[0] <= merged_nums[-1]:
                      return None, "merged_not_strictly_increasing_after_shift"

                  merged_rows.append(row)
                  row_offsets.append(offset)
                  merged_nums.extend(shifted)
                  merged_raw_labels.extend(list(raw_starts))
                  merged_warnings.extend(list(row_warnings))
                  raw_segments.append(
                      {
                          "path": row.get("path"),
                          "movement_index": int(row.get("movement_index") or 1),
                          "system_starts": list(raw_starts),
                      }
                  )

                  if len(merged_nums) == target_system_count:
                      return {
                          "kind": "merge_shifted_exact" if any_shift else "merge_exact",
                          "rows": list(merged_rows),
                          "system_starts": [str(n) for n in merged_nums],
                          "raw_system_starts": [str(x) for x in merged_raw_labels],
                          "system_starts_normalized": [str(n) for n in merged_nums],
                          "normalization_warnings": list(merged_warnings),
                          "row_offsets": list(row_offsets),
                          "system_starts_raw_segments": list(raw_segments),
                      }, None
                  if len(merged_nums) > target_system_count:
                      return None, "merged_system_count_exceeds_target"

              return None, "merged_system_count_below_target"

          def merge_measure_events(rows, offsets):
              merged = []
              for row, offset in zip(rows, offsets):
                  row_events = list(row.get("measure_events") or [])
                  for ev in row_events:
                      ev_num_int = ev.get("number_int")
                      shifted_num_int = (int(ev_num_int) + int(offset)) if ev_num_int is not None else None
                      merged.append(
                          {
                              "label": str(ev.get("label") or ""),
                              "number_int": shifted_num_int,
                              "implicit": bool(ev.get("implicit")),
                              "system_start": bool(ev.get("system_start")),
                              "repeat_forward": bool(ev.get("repeat_forward")),
                              "repeat_backward": bool(ev.get("repeat_backward")),
                              "ending_start_numbers": list(ev.get("ending_start_numbers") or []),
                          }
                      )
              return merged

          def summarize_semantic_events(events):
              pickup = bool(events and events[0].get("implicit"))
              repeats_forward = sum(1 for ev in events if ev.get("repeat_forward"))
              repeats_backward = sum(1 for ev in events if ev.get("repeat_backward"))
              endings = sum(1 for ev in events if ev.get("ending_start_numbers"))
              return {
                  "pickup_detected": pickup,
                  "repeat_forward_count": repeats_forward,
                  "repeat_backward_count": repeats_backward,
                  "ending_start_count": endings,
              }

          def estimate_page_tail_increment(raw_labels, measures_count):
              nums, _ = parse_int_labels(raw_labels or [], allow_normalize=True)
              if not nums:
                  return 1
              mc = int(measures_count or 0)
              if mc > 0:
                  span = nums[-1] - nums[0]
                  est = mc - span
                  if est > 0:
                      return int(est)
              if len(nums) >= 2:
                  return max(1, nums[-1] - nums[-2])
              return 1

          def apply_semantic_numbering(entries, numbering_policy):
              if numbering_policy == "legacy_raw_starts":
                  for entry in entries:
                      if entry.get("status") != "ok":
                          entry["numbering_policy"] = "legacy_raw_starts"
                          continue
                      raw_starts = list(entry.get("system_starts_raw") or entry.get("system_starts") or [])
                      entry["system_starts_semantic"] = list(raw_starts)
                      entry["system_starts"] = list(raw_starts)
                      entry["numbering_policy"] = "legacy_raw_starts"
                      print(
                          "SEMANTIC_APPLY "
                          f"page={entry.get('page_number')} source=legacy_raw_starts "
                          f"raw_starts={raw_starts} semantic_starts={raw_starts}"
                      )
                  return

              global_next_start = None
              seen_first_page = False
              last_first_ending_value = None

              for entry in sorted(entries, key=lambda e: int(e.get("page_number") or 0)):
                  if entry.get("status") != "ok":
                      entry["numbering_policy"] = "semantic_continuous_v1"
                      continue

                  raw_starts = list(entry.get("system_starts_raw") or entry.get("system_starts") or [])
                  normalized_starts = list(entry.get("system_starts_normalized") or entry.get("system_starts") or [])
                  raw_nums, parse_warnings = parse_int_labels(normalized_starts, allow_normalize=False)
                  if parse_warnings:
                      entry.setdefault("semantic_warnings", []).extend(parse_warnings)
                  if not raw_nums:
                      entry.setdefault("semantic_warnings", []).append("raw_system_starts_non_numeric_or_empty")
                      entry["system_starts_semantic"] = list(normalized_starts)
                      entry["system_starts"] = list(normalized_starts)
                      entry["numbering_policy"] = "semantic_continuous_v1"
                      print(
                          "SEMANTIC_WARN "
                          f"page={entry.get('page_number')} reason=raw_system_starts_non_numeric_or_empty"
                      )
                      print(
                          "SEMANTIC_APPLY "
                          f"page={entry.get('page_number')} source=semantic_continuous_v1 "
                          f"raw_starts={raw_starts} semantic_starts={entry['system_starts_semantic']}"
                      )
                      continue

                  deltas = [max(1, b - a) for a, b in zip(raw_nums, raw_nums[1:])]
                  events = list(entry.get("_semantic_measure_events") or [])
                  summary = dict(entry.get("semantic_events") or {})
                  pickup_detected = bool(summary.get("pickup_detected"))
                  if not seen_first_page:
                      start_value = 0 if pickup_detected else int(raw_nums[0])
                  else:
                      start_value = int(global_next_start) if global_next_start is not None else int(raw_nums[0])

                  semantic_nums = [start_value]
                  for delta in deltas:
                      semantic_nums.append(int(semantic_nums[-1]) + int(delta))

                  start_events = [ev for ev in events if ev.get("system_start")]
                  if len(start_events) != len(semantic_nums):
                      entry.setdefault("semantic_warnings", []).append(
                          f"system_start_event_count_mismatch:{len(start_events)}!={len(semantic_nums)}"
                      )
                      print(
                          "SEMANTIC_WARN "
                          f"page={entry.get('page_number')} reason=system_start_event_count_mismatch "
                          f"events={len(start_events)} systems={len(semantic_nums)}"
                      )
                  else:
                      for idx, ev in enumerate(start_events):
                          ending_nums = set(str(x) for x in (ev.get("ending_start_numbers") or []))
                          if "1" in ending_nums:
                              last_first_ending_value = semantic_nums[idx]
                          elif "2" in ending_nums and last_first_ending_value is not None:
                              semantic_nums[idx] = int(last_first_ending_value)

                  semantic_starts = [str(n) for n in semantic_nums]
                  entry["system_starts_semantic"] = semantic_starts
                  entry["system_starts"] = semantic_starts
                  entry["numbering_policy"] = "semantic_continuous_v1"

                  tail_increment = estimate_page_tail_increment(normalized_starts, entry.get("measures_count"))
                  global_next_start = int(semantic_nums[-1]) + int(tail_increment)
                  seen_first_page = True

                  print(
                      "SEMANTIC_APPLY "
                      f"page={entry.get('page_number')} source=semantic_continuous_v1 "
                      f"raw_starts={raw_starts} semantic_starts={semantic_starts}"
                  )

          page_pdfs = sorted(glob.glob("/tmp/work/pages/page_*.pdf"))
          entries = []
          missing = 0
          omr_targets, omr_target_status, omr_target_error = load_omr_system_targets(OMR_PATH, len(page_pdfs))

          print(
              "PAGE_XML_TARGETS "
              f"omr_path={OMR_PATH or '-'} status={omr_target_status} "
              f"error={omr_target_error} pages={len(omr_targets)}"
          )

          for page_pdf in page_pdfs:
              page_num = int(re.search(r"page_(\d{4})\.pdf$", page_pdf).group(1))
              page_idx = page_num - 1
              out_dir = f"/tmp/work/audiveris_pages/page_{page_num:04d}"
              candidates = sorted(glob.glob(os.path.join(out_dir, "**", "*.mxl"), recursive=True))
              scored = [score_mxl(path) for path in candidates]
              candidate_summary = []
              for row in scored:
                  candidate_summary.append(
                      {
                          "path": row.get("path"),
                          "movement_index": int(row.get("movement_index") or 1),
                          "member": row.get("member"),
                          "parts": int(row.get("parts") or 0),
                          "measures": int(row.get("measures") or 0),
                          "systems": len(row.get("system_starts") or []),
                          "parse": row.get("parse"),
                          "error": row.get("error"),
                      }
                  )
                  print(
                      "PAGE_XML_CANDIDATE "
                      f"page={page_num} path={row.get('path')} movement={int(row.get('movement_index') or 1)} "
                      f"systems={len(row.get('system_starts') or [])} parts={int(row.get('parts') or 0)} "
                      f"measures={int(row.get('measures') or 0)} member={row.get('member')}"
                  )

              valid = [
                  row
                  for row in scored
                  if int(row.get("parts") or 0) > 0
                  and int(row.get("measures") or 0) > 0
                  and len(row.get("system_starts") or []) > 0
              ]
              target = int(omr_targets.get(page_num) or 0)
              print(
                  "PAGE_XML_TARGET "
                  f"page={page_num} omr_systems={target} candidates={len(scored)} valid_candidates={len(valid)}"
              )

              selected, select_error = select_manifest_entry(valid, target)

              if selected is not None:
                  used_rows = selected["rows"]
                  used_paths = [row["path"] for row in used_rows]
                  used_movement_indices = [int(row.get("movement_index") or 1) for row in used_rows]
                  system_starts = list(selected["system_starts"])
                  raw_system_starts = list(selected.get("raw_system_starts") or system_starts)
                  system_starts_normalized = list(selected.get("system_starts_normalized") or system_starts)
                  normalization_warnings = list(selected.get("normalization_warnings") or [])
                  row_offsets = list(selected.get("row_offsets") or [0 for _ in used_rows])
                  raw_segments = list(selected.get("system_starts_raw_segments") or [])
                  normalized_count = sum(
                      1 for warn in normalization_warnings if str(warn).startswith("normalized_suffix_label:")
                  )
                  print(
                      "PAGE_XML_NORMALIZE "
                      f"page={page_num} raw={raw_system_starts} normalized={system_starts_normalized} "
                      f"normalized_count={normalized_count}"
                  )
                  for warn in normalization_warnings:
                      print(f"PAGE_XML_NORMALIZE_WARN page={page_num} reason={warn}")
                  merge_offsets = []
                  for row, offset in zip(used_rows, row_offsets):
                      raw_nums, _ = parse_int_labels(list(row.get("system_starts") or []), allow_normalize=True)
                      raw_nums = raw_nums or []
                      raw_first = raw_nums[0] if raw_nums else None
                      raw_last = raw_nums[-1] if raw_nums else None
                      shifted_first = (raw_first + int(offset)) if raw_first is not None else None
                      shifted_last = (raw_last + int(offset)) if raw_last is not None else None
                      print(
                          "PAGE_XML_SHIFT "
                          f"page={page_num} path={row.get('path')} movement={int(row.get('movement_index') or 1)} "
                          f"offset={int(offset)} raw_first={raw_first} raw_last={raw_last} "
                          f"shifted_first={shifted_first} shifted_last={shifted_last}"
                      )
                      merge_offsets.append(
                          {
                              "path": row.get("path"),
                              "movement_index": int(row.get("movement_index") or 1),
                              "offset": int(offset),
                          }
                      )
                  merged_events = merge_measure_events(used_rows, row_offsets)
                  semantic_events = summarize_semantic_events(merged_events)
                  semantic_events["ending_label_normalized_count"] = int(normalized_count)
                  print(
                      "SEMANTIC_DETECT "
                      f"page={page_num} pickup={str(semantic_events.get('pickup_detected')).lower()} "
                      f"repeats_forward={int(semantic_events.get('repeat_forward_count') or 0)} "
                      f"repeats_backward={int(semantic_events.get('repeat_backward_count') or 0)} "
                      f"endings={int(semantic_events.get('ending_start_count') or 0)}"
                  )
                  primary = used_rows[0]
                  if selected["kind"] == "single_exact":
                      selection_source = "per_page_split_single_exact"
                      selection_reason = "single_candidate_exact_system_count"
                      parts_count = int(primary["parts"])
                      measures_count = int(primary["measures"])
                      mxl_member = primary.get("member")
                  elif selected["kind"] == "merge_shifted_exact":
                      selection_source = "per_page_split_merge_shifted_exact"
                      selection_reason = "merged_reset_shifted_exact_system_count"
                      parts_count = max(int(row.get("parts") or 0) for row in used_rows)
                      measures_count = sum(int(row.get("measures") or 0) for row in used_rows)
                      mxl_member = None
                  else:
                      selection_source = "per_page_split_merge_exact"
                      selection_reason = "merged_candidates_exact_system_count"
                      parts_count = max(int(row.get("parts") or 0) for row in used_rows)
                      measures_count = sum(int(row.get("measures") or 0) for row in used_rows)
                      mxl_member = None

                  entry = {
                      "page_index": page_idx,
                      "page_number": page_num,
                      "status": "ok",
                      "error": None,
                      "mxl_path": primary["path"],
                      "mxl_paths": used_paths,
                      "mxl_member": mxl_member,
                      "movement_indices": used_movement_indices,
                      "parts_count": parts_count,
                      "measures_count": measures_count,
                      "system_count_target": target,
                      "system_starts_raw": list(raw_system_starts),
                      "system_starts_normalized": list(system_starts_normalized),
                      "system_starts_semantic": list(system_starts_normalized),
                      "system_starts": list(system_starts_normalized),
                      "merge_offsets": merge_offsets,
                      "system_starts_raw_segments": raw_segments,
                      "numbering_policy": NUMBERING_POLICY,
                      "semantic_events": semantic_events,
                      "semantic_warnings": [],
                      "normalization_warnings": normalization_warnings,
                      "_semantic_measure_events": merged_events,
                      "selection_source": selection_source,
                      "selection_reason": selection_reason,
                      "confidence_tier": "high",
                      "low_confidence_used": False,
                      "retry_attempted": False,
                      "retry_profile": "none",
                      "candidate_summary": candidate_summary,
                  }
                  print(
                      "PAGE_XML_SELECTED "
                      f"page={page_num} source={selection_source} systems={len(system_starts)} "
                      f"target={target} paths={used_paths}"
                  )
              else:
                  if not scored:
                      reason = "no_mxl_candidates"
                  elif target <= 0:
                      reason = "omr_system_target_missing"
                  elif not valid:
                      reason = "parts_or_measures_or_system_starts_invalid"
                  else:
                      reason = "system_count_mismatch"

                  best = max(
                      scored,
                      key=lambda row: (
                          int(row.get("measures") or 0),
                          int(row.get("parts") or 0),
                          row.get("path") or "",
                          row.get("member") or "",
                      ),
                  ) if scored else None
                  best_raw_starts = list(best.get("system_starts") or []) if best else []
                  best_norm_nums, best_norm_warnings = parse_int_labels(best_raw_starts, allow_normalize=True)
                  best_norm_starts = [str(n) for n in (best_norm_nums or [])]
                  best_norm_count = sum(
                      1 for warn in best_norm_warnings if str(warn).startswith("normalized_suffix_label:")
                  )
                  if best or best_norm_warnings:
                      print(
                          "PAGE_XML_NORMALIZE "
                          f"page={page_num} raw={best_raw_starts} normalized={best_norm_starts} "
                          f"normalized_count={best_norm_count}"
                      )
                  for warn in best_norm_warnings:
                      print(f"PAGE_XML_NORMALIZE_WARN page={page_num} reason={warn}")
                  entry = {
                      "page_index": page_idx,
                      "page_number": page_num,
                      "status": "missing",
                      "error": reason,
                      "mxl_path": best.get("path") if best else None,
                      "mxl_paths": [row.get("path") for row in scored],
                      "mxl_member": best.get("member") if best else None,
                      "movement_indices": [int(row.get("movement_index") or 1) for row in scored],
                      "parts_count": int(best.get("parts") or 0) if best else 0,
                      "measures_count": int(best.get("measures") or 0) if best else 0,
                      "system_count_target": target,
                      "system_starts_raw": best_raw_starts,
                      "system_starts_normalized": best_norm_starts,
                      "system_starts_semantic": best_norm_starts,
                      "system_starts": best_norm_starts,
                      "merge_offsets": [],
                      "system_starts_raw_segments": [],
                      "numbering_policy": NUMBERING_POLICY,
                      "semantic_events": {
                          "pickup_detected": False,
                          "repeat_forward_count": 0,
                          "repeat_backward_count": 0,
                          "ending_start_count": 0,
                          "ending_label_normalized_count": int(best_norm_count),
                      },
                      "semantic_warnings": [],
                      "normalization_warnings": list(best_norm_warnings),
                      "_semantic_measure_events": [],
                      "selection_source": "none",
                      "selection_reason": select_error or reason,
                      "confidence_tier": "none",
                      "low_confidence_used": False,
                      "retry_attempted": False,
                      "retry_profile": "none",
                      "candidate_summary": candidate_summary,
                  }
                  missing += 1
                  print(
                      "PAGE_XML_UNRESOLVED "
                      f"page={page_num} reason={reason} target={target} "
                      f"valid_candidates={len(valid)} select_error={select_error}"
                  )
              entries.append(entry)

          print(f"SEMANTIC_POLICY effective={NUMBERING_POLICY}")
          apply_semantic_numbering(entries, NUMBERING_POLICY)
          for entry in entries:
              if "_semantic_measure_events" in entry:
                  del entry["_semantic_measure_events"]

          payload = {
              "manifest_version": "page-split-v2",
              "numbering_policy": NUMBERING_POLICY,
              "entries": entries,
              "total_pages": len(entries),
              "missing": missing,
              "omr_system_targets": omr_targets,
              "omr_system_targets_status": omr_target_status,
              "omr_system_targets_error": omr_target_error,
          }
          with open(MANIFEST_PATH, "w", encoding="utf-8") as f:
              json.dump(payload, f, indent=2)

          print(
              f"MXL_PAGE_MANIFEST path={MANIFEST_PATH} entries={len(entries)} missing={missing}"
          )
          PY

          echo "MXL_PAGE_MANIFEST_PATH=/tmp/work/mxl_page_manifest.json" >> "$GITHUB_ENV"

      - name: Annotate PDF (guides + measure labels) with verbose debug
        env:
          DEBUG_GUIDES: "0"
          ENABLE_MEASURE_LABELS: "1"
          MEASURE_LABEL_MODE: "staff_start"
          MEASURE_SOURCE_POLICY: "mxl_strict"
          MXL_PARSER_POLICY: "auto"
          MXL_SANITIZE_PREFIXES: "1"
          MXL_PATH_OVERRIDE: "${{ env.MXL_PATH_OVERRIDE }}"
          MXL_MEMBER_OVERRIDE: "${{ env.MXL_MEMBER_OVERRIDE }}"
          MXL_PAGE_MANIFEST_PATH: "${{ env.MXL_PAGE_MANIFEST_PATH }}"
          DEBUG_MEASURE_LABELS: "1"
          DEBUG_MEASURE_MARKERS: "0"
          DEBUG_SENTINEL_TEXT: "1"
          MEASURE_MAPPING_DEBUG_PATH: "/tmp/work/measure_mapping_debug.json"
        run: |
          set -euo pipefail
          echo "PARSER_SHA=$(git rev-parse --short HEAD)"
          python - << 'PY'
          p = "parser-api/annotate_guides_from_omr.py"
          text = open(p, "r", encoding="utf-8").read()
          markers = [
              "mxl_movement",
              "mapping_mode",
              "labels_to_draw_count",
              "DEBUG_SENTINEL_TEXT",
              "MXL_PAGE_MANIFEST_PATH",
          ]
          for m in markers:
              print(f"PARSER_MARKER {m} present={m in text}")
          PY
          python -u parser-api/annotate_guides_from_omr.py \
            /tmp/work/input.pdf \
            "${AUDI_OMR_PATH}" \
            /tmp/work/annotated.pdf
          ls -lh /tmp/work/annotated.pdf

      - name: Sanity-check output PDF
        run: |
          set -euo pipefail
          python - << 'PY'
          import fitz

          in_doc = fitz.open('/tmp/work/input.pdf')
          out_doc = fitz.open('/tmp/work/annotated.pdf')
          print('input_pages=', in_doc.page_count)
          print('output_pages=', out_doc.page_count)
          if in_doc.page_count != out_doc.page_count:
            raise SystemExit('ERROR: output page count mismatch')
          PY

      - name: Mapping debug summary
        run: |
          set -euo pipefail
          python - << 'PY'
          import json

          p = "/tmp/work/measure_mapping_debug.json"
          d = json.load(open(p, "r", encoding="utf-8"))
          print("SUMMARY mxl_path=", d.get("mxl_path"))
          print("SUMMARY mxl_paths=", d.get("mxl_paths"))
          movements = d.get("mxl_movements") or []
          for i, m in enumerate(movements, start=1):
              print(
                  f"SUMMARY movement#{i} path={m.get('mxl_path')} "
                  f"status={m.get('mxl_parse_status')} pages={m.get('mxl_pages')} "
                  f"member={m.get('mxl_member_path')} "
                  f"source_sheets={m.get('mxl_source_sheet_numbers')}"
              )

          pages = d.get("pages") or []
          manifest_path = d.get("mxl_page_manifest_path")
          if manifest_path:
              manifest_summary = d.get("mxl_page_manifest_summary") or {}
              print(f"SUMMARY manifest_path={manifest_path}")
              print(
                  "SUMMARY manifest "
                  f"entries={manifest_summary.get('entries')} "
                  f"missing={manifest_summary.get('missing')} "
                  f"status={manifest_summary.get('status')}"
              )
          strict_xml_pages_ok = 0
          total_pages = len(pages)
          strict_xml_missing = []
          for pg in pages:
              source = pg.get("staff_start_source")
              status = pg.get("mapping_status")
              mode = pg.get("mapping_mode")
              draw_attempted = bool(pg.get("draw_attempted"))
              labels_to_draw = int(pg.get("labels_to_draw_count") or 0)
              labels_drawn = int(pg.get("labels_drawn") or 0)
              labels_in_bounds = int(pg.get("labels_in_bounds") or 0)

              if source != "mxl":
                  if mode == "missing":
                      failure_class = "NO_XML_COVERAGE"
                  elif status == "error":
                      failure_class = "MAP_EMPTY"
                  else:
                      failure_class = "MAP_NOT_MXL"
              elif labels_to_draw == 0:
                  failure_class = "DRAW_EMPTY"
              elif labels_drawn == 0:
                  failure_class = "DRAW_ZERO"
              elif labels_in_bounds == 0:
                  failure_class = "DRAW_OFFPAGE"
              else:
                  failure_class = "DRAW_OK"

              if source == "mxl":
                  strict_xml_pages_ok += 1
              else:
                  strict_xml_missing.append(
                      (
                          int(pg.get("page_index", 0)) + 1,
                          str(pg.get("mapping_reason")),
                          str(status),
                          str(mode),
                      )
                  )

              print(
                  f"SUMMARY page={pg.get('page_index', 0)+1} "
                  f"source={source} "
                  f"status={status} "
                  f"reason={pg.get('mapping_reason')} "
                  f"mode={mode} "
                  f"selection_source={pg.get('xml_selection_source')} "
                  f"confidence={pg.get('xml_confidence_tier')} "
                  f"mxl_sys={len(pg.get('mxl_page_system_starts') or [])} "
                  f"omr_sys={len(pg.get('omr_system_staff_counts') or [])} "
                  f"assigned={len(pg.get('assigned_labels') or [])} "
                  f"candidates={pg.get('staff_start_candidate_count')} "
                  f"draw_attempted={draw_attempted} "
                  f"labels_to_draw={labels_to_draw} "
                  f"labels_drawn={labels_drawn} "
                  f"labels_in_bounds={labels_in_bounds} "
                  f"failure_class={failure_class}"
              )

          print(f"SUMMARY strict_xml_pages_ok={strict_xml_pages_ok}/{total_pages}")
          if strict_xml_pages_ok != total_pages:
              for page_no, reason, status, mode in strict_xml_missing:
                  print(
                      "SUMMARY strict_xml_missing "
                      f"page={page_no} reason={reason} status={status} mode={mode}"
                  )
              raise SystemExit(
                  f"ERROR: strict XML coverage failed strict_xml_pages_ok={strict_xml_pages_ok}/{total_pages}"
              )
          PY

      - name: Bundle debug artifacts
        if: always()
        run: |
          set -euo pipefail
          rm -f /tmp/work/omr_debug_bundle.tar.gz
          tar -czf /tmp/work/omr_debug_bundle.tar.gz \
            /tmp/work/audi_help_probe.txt \
            /tmp/work/audi_help.txt \
            /tmp/work/selected_mxl_path*.txt \
            /tmp/work/selected_mxl_member*.txt \
            /tmp/work/audiveris_smoke \
            /tmp/work/audiveris_out_try1 \
            /tmp/work/pages \
            /tmp/work/page_logs \
            /tmp/work/audiveris_pages \
            /tmp/work/mxl_page_manifest.json \
            /tmp/work/measure_mapping_debug.json \
            2>/dev/null || true
          if [[ -f /tmp/work/omr_debug_bundle.tar.gz ]]; then
            ls -lh /tmp/work/omr_debug_bundle.tar.gz
          fi

      - name: Upload outputs and debug artifacts
        if: always()
        run: |
          set -euo pipefail
          RUN_OUTPUT_PREFIX="${OUTPUT_PREFIX}/runs/${GITHUB_RUN_ID}"
          RUN_REF="${GITHUB_REF_NAME:-$GITHUB_REF}"
          echo "RUN_SUMMARY run_id=${GITHUB_RUN_ID} run_attempt=${GITHUB_RUN_ATTEMPT} ref=${RUN_REF} sha=${GITHUB_SHA}"
          echo "RUN_SUMMARY output_prefix=${OUTPUT_PREFIX}"
          echo "RUN_SUMMARY run_output_prefix=${RUN_OUTPUT_PREFIX}"

          if [[ -f /tmp/work/annotated.pdf ]]; then
            gsutil cp /tmp/work/annotated.pdf "${RUN_OUTPUT_PREFIX}/annotated.pdf"
            echo "uploaded_run_annotated=${RUN_OUTPUT_PREFIX}/annotated.pdf"
            gsutil cp /tmp/work/annotated.pdf "${OUTPUT_PREFIX}/annotated.pdf"
            echo "uploaded_latest=${OUTPUT_PREFIX}/annotated.pdf"
            echo "RUN_SUMMARY artifact_annotated_run=${RUN_OUTPUT_PREFIX}/annotated.pdf"
            echo "RUN_SUMMARY artifact_annotated_latest=${OUTPUT_PREFIX}/annotated.pdf"
          else
            echo "WARN: /tmp/work/annotated.pdf not found"
            echo "RUN_SUMMARY artifact_annotated_run=missing"
            echo "RUN_SUMMARY artifact_annotated_latest=missing"
          fi
          if [[ -f /tmp/work/measure_mapping_debug.json ]]; then
            gsutil cp /tmp/work/measure_mapping_debug.json "${RUN_OUTPUT_PREFIX}/measure_mapping_debug.json"
            gsutil cp /tmp/work/measure_mapping_debug.json "${OUTPUT_PREFIX}/measure_mapping_debug.json"
            echo "uploaded_mapping_debug=${OUTPUT_PREFIX}/measure_mapping_debug.json"
            echo "uploaded_run_mapping_debug=${RUN_OUTPUT_PREFIX}/measure_mapping_debug.json"
            echo "RUN_SUMMARY artifact_mapping_debug=${RUN_OUTPUT_PREFIX}/measure_mapping_debug.json"
            echo "RUN_SUMMARY artifact_mapping_debug_latest=${OUTPUT_PREFIX}/measure_mapping_debug.json"
          else
            echo "WARN: /tmp/work/measure_mapping_debug.json not found"
            echo "RUN_SUMMARY artifact_mapping_debug=missing"
            echo "RUN_SUMMARY artifact_mapping_debug_latest=missing"
          fi
          if [[ -f /tmp/work/mxl_page_manifest.json ]]; then
            gsutil cp /tmp/work/mxl_page_manifest.json "${RUN_OUTPUT_PREFIX}/mxl_page_manifest.json"
            gsutil cp /tmp/work/mxl_page_manifest.json "${OUTPUT_PREFIX}/mxl_page_manifest.json"
            echo "uploaded_manifest=${OUTPUT_PREFIX}/mxl_page_manifest.json"
            echo "uploaded_run_manifest=${RUN_OUTPUT_PREFIX}/mxl_page_manifest.json"
            echo "RUN_SUMMARY artifact_manifest=${RUN_OUTPUT_PREFIX}/mxl_page_manifest.json"
            echo "RUN_SUMMARY artifact_manifest_latest=${OUTPUT_PREFIX}/mxl_page_manifest.json"
          else
            echo "WARN: /tmp/work/mxl_page_manifest.json not found"
            echo "RUN_SUMMARY artifact_manifest=missing"
            echo "RUN_SUMMARY artifact_manifest_latest=missing"
          fi
          if [[ -f /tmp/work/omr_debug_bundle.tar.gz ]]; then
            gsutil cp /tmp/work/omr_debug_bundle.tar.gz "${RUN_OUTPUT_PREFIX}/omr_debug_bundle.tar.gz"
            gsutil cp /tmp/work/omr_debug_bundle.tar.gz "${OUTPUT_PREFIX}/omr_debug_bundle.tar.gz"
            echo "uploaded_debug_bundle=${OUTPUT_PREFIX}/omr_debug_bundle.tar.gz"
            echo "uploaded_run_debug_bundle=${RUN_OUTPUT_PREFIX}/omr_debug_bundle.tar.gz"
            echo "RUN_SUMMARY artifact_debug_bundle=${RUN_OUTPUT_PREFIX}/omr_debug_bundle.tar.gz"
            echo "RUN_SUMMARY artifact_debug_bundle_latest=${OUTPUT_PREFIX}/omr_debug_bundle.tar.gz"
          else
            echo "WARN: /tmp/work/omr_debug_bundle.tar.gz not found"
            echo "RUN_SUMMARY artifact_debug_bundle=missing"
            echo "RUN_SUMMARY artifact_debug_bundle_latest=missing"
          fi

          echo "Upload complete"
